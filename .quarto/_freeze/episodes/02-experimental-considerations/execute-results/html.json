{
  "hash": "61384532e017af3f70cfa42fad39e71a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Before the experiment\"\nformat:\n    html:\n        fig-width: 5\n        fig-height: 4\n        code-annotations: hover\n---\n\n\n\n\nBefore diving into the bioinformatics of RNA-seq experiments, it's good to take a step back and think about experimental design and refresh your statistical knowledge. After all, an RNA-seq experiment is also an experiment like any other.\n\n## Hypothesis testing\n\nIn an RNA-seq experiment, you will analyze the expression levels of thousands of genes. Before doing that, let us consider statistical tests and their associated p-values  for a single gene. Say you are studying a plant gene called \"heat stress trancripion factor A-2 (HSFA2)\", whose expression might be induced by.... heat stress (you might have guessed that). You could phrase your *null hypothesis*: \n\n> \"The average HSFA2 gene expression is *the same* in normal conditions than under heat stress in my Arabidopsis seedling\" \n\nThe corresponding *alternative hypothesis* could then be:\n\n> \"The average HSFA2 gene expression is different under heat stress compared to normal conditions in my Arabidopsis seedlings\"\n\nYou go into the greenhouse, sample 20 *Arabidopsis thaliana* plants in both control and heat stressed conditions, and measure the expression of HSF2A. Let's visualize this in a boxplot made in `R`, using the `ggplot2` package.\n\n\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nset.seed(1992)\n\ncontrol <- tibble(expression = rnorm(n = 20, mean = 2, sd = 0.6), \n                    condition = \"control\")\nheat_stress <- tibble(expression = rnorm(n = 20, mean = 3.4, sd = 0.3),\n                    condition = \"heat stress\") \n                         \nexperiment <- bind_rows(control, heat_stress)\n\nggplot(experiment, aes(x = condition, y = expression)) + \n    geom_boxplot(aes(fill = condition), alpha = 0.5, outlier.shape = NA) +\n    geom_jitter(width = 0.2, alpha = 0.5) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](02-experimental-considerations_files/figure-html/unnamed-chunk-1-1.png){width=480}\n:::\n:::\n\n\n\n\nIndeed, HSF2A is highly expressed in heat stressed plants. The thick line of the boxplots show the *median* HSF2A expression in both groups. The median is a measure of central tendency. You will also see that the individual datapoints are dispersed around the boxplot: quantifying this gives us a measure of the *spread* of the data. An example of such a measure of spread is the *standard deviation*. It looks like our heat stressed plants display a more narrow spread of the data.\n\n::: {.callout-tip title=\"Question\" icon=\"false\"}\nCan you name another measure of central tendency? And another one for the measure of spread?\n:::\n\n::: {.callout-caution title=\"Answer\" collapse=\"true\" icon=\"false\"}\nAnother often used measure of central tendency would be the *mean*. As for the measure of spread, the variance (square of the standard deviation) or the standard error (standard deviation divided by the square root of the number of observations) are often used. \n:::\n\nLet's test whether the *means* of the two groups are equal or not. We do this with a **two-sample *t*-test**.\n\n\n\n\n::: {.cell filename='R'}\n\n```{.r .cell-code}\nt.test(x = control$expression,\n       y = heat_stress$expression, # <1>\n       alternative = \"two.sided\" ,      \n       var.equal = FALSE,             # important as the variance do not seem equal\n       conf.level = 0.95)             # this corresponds to alpha = 0.05 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  control$expression and heat_stress$expression\nt = -12.237, df = 25.636, p-value = 3.312e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.600116 -1.139603\nsample estimates:\nmean of x mean of y \n 1.970023  3.339883 \n```\n\n\n:::\n:::\n\n\n\n1. lala;a\n\nBecause the **p-value < 0.05**, we can reject our null hypothesis. So, we have discovered that *Arabidopsis* significantly upregulates the expression of HSF2A under heat stress. Sounds like it's time to write this down and submit a manuscript to *Nature*!\n\n### Statistical power\n\nStatistical power is the ability of your experiment to detect differences that actually exist (in gene expression, in this case). There are two main error one could make when testing hypotheses. Type I errors occur when the null hypothesis is rejected wrongly (i.e., you detect a significant difference while in reality there is no difference in gene expression). Type II errors also are common in hypothesis testing: they arise if you accept the null hypothesis when in fact a treatment effect exists. \n\nThe power of an experiments is affected by several factors. We have seen so far:\n\n- **The number of replicates**: more samples results in a higher power\n- **Size of the effect**: bigger changes are easier to detect\n- **Variability of the data**: less noise is more power\n- **Confidence level threshold**: usually, this is 0.05, but we can make our analysis more or less stringent by changing this to 0.01 or 0.1\n\n## What determines the power of an RNA-seq experiment?\n\nRNA-seq experiments measure the expression levels of thousands of genes simultaneously, meaning we perform thousands of statistical tests at once. To avoid making too many false discoveries (type I errors), we correct for multiple testing. However, it also reduces statistical power -- increasing the chance of missing real differences (type II errors). As a result, RNA-seq experiments can have lower power, especially for detecting small or subtle changes in gene expression. \n\nSo, how do we deal with that? We have a few parameters to play with when designing an RNA-seq experiment, including deciding the number of reads per sample, and the number of samples per treatment. [Liu et al., 2014](https://academic.oup.com/bioinformatics/article/30/3/301/228651) investigated this systematically in an RNA-seq experiment with a human cell line. An exerpt from figure 1 is show below:\n\n![](../images/02-de-replicates-2.png){width=5in}\n\n*Image credits*: [Liu et al., 2014](https://academic.oup.com/bioinformatics/article/30/3/301/228651)\n\nThe figure shows the relationship between sequencing depth and number of replicates on the number of differentially expressed genes (DEGs) identified. It illustrates that **biological replicates are of greater importance than sequencing depth**. Above 10M-15M reads per sample, the number of DEGs increases marginally, while adding more biological replicates tends to return more DEGs. So, if your experiment is limited by a certain sequencing budget, it is almost always better to add more replicates than to sequence more reads of a limited number of samples. TODO: add potential caveats here?\n\n## Experimental design\n\nIn a typical biological experiment, you will encounter various sources of variation that are either:\n\n- **Desirable** because they are part of your experimental factors. For example, the genotype, treatment, or timepoint.\n- **Undesirable** because you are not interested in them. This could be: batch of RNA isolation, location of plants in the greenhouse, ...\n\nUndesirable variation is unavaidable, but there are some practices to limit their impact. First of all, make sure that you don't confound your experimental factors with undesirable sources of variation. For example, if you have too many samples to isolate RNA in one day of labwork, make sure you don't isolate RNA from samples with genotype A on day 1, and genotype B on day 2. In this case, you will never be able to know whether genes were differentially expressed because the genotypes of the samples differ (very interesting!) or because RNA was isolated on two different days (not very interesting). Instead, mix your samples of genotype A and B over your two RNA isolation days. Still, you should always record known sources of undesirable variation so you can correct for it later. There's much more to be said about this, for more details see [these materials from Harvard Chan Bioinformatics Core](https://hbctraining.github.io/Intro-to-rnaseq-fasrc-salmon-flipped/lessons/02_experimental_planning_considerations.html)\n\n\n## Concluding remarks\n\nIn this section, we refreshed our statistics knowledge, and discussed how this applies to RNA-seq experiments. Hopefully, this enables you to craft a well-designed and controlled RNA-seq experiment. Now, head to the lab or greenhouse, perform your experiment, and extract high-quality RNA. How to do this will depend on your study system, and is beyond the scope of this workshop. We will see you again when you get the sequencing reads from the sequencing provider. We will then jump into the bioinformatics pipeline required to check the quality of the reads, and map the reads to the genome of your organisms of interest.\n",
    "supporting": [
      "02-experimental-considerations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}