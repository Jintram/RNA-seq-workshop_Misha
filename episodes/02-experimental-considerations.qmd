---
title: "Before the experiment"
format:
    html:
        fig-width: 5
        fig-height: 4
        code-annotations: hover
---

Before diving into the bioinformatics of RNA-seq experiments, it's good to take a step back and think about experimental design and refresh your statistical knowledge. After all, an RNA-seq experiment is also an experiment like any other.

## Hypothesis testing

In an RNA-seq experiment, you will analyze the expression levels of thousands of genes. Before doing that, let us consider statistical tests and their associated p-values for a single gene. Let's say you are studying a plant gene called "heat stress trancripion factor A-2 (HSFA2)", whose expression might be induced by.... heat stress (you might have guessed that). You could phrase your *null hypothesis*: 

> "The average HSFA2 gene expression is *the same* in normal conditions and under heat stress in my Arabidopsis seedlings" 

The corresponding *alternative hypothesis* could then be:

> "The average HSFA2 gene expression is different under heat stress compared to normal conditions in my Arabidopsis seedlings"

You set up an experiment in the greenhouse. You grow *Arabidopsis thaliana* seedlings, and give half of them a heat stress treatment. Then, you take samples of 20 plants in both control and heat stressed conditions, and measure the expression levels of HSF2A. Let's visualize this in a boxplot made in `R`, using the `ggplot2` package.

```{R filename = "R"}
#| warning: false
library(tidyverse)
set.seed(1992)

control <- tibble(expression = rnorm(n = 20, mean = 2, sd = 0.6), 
                    condition = "control")
heat_stress <- tibble(expression = rnorm(n = 20, mean = 3.4, sd = 0.3),
                    condition = "heat stress") 
                         
experiment <- bind_rows(control, heat_stress)

ggplot(experiment, aes(x = condition, y = expression)) + 
    geom_boxplot(aes(fill = condition), alpha = 0.5, outlier.shape = NA) +
    geom_jitter(width = 0.2, alpha = 0.5) +
    theme_classic()
```

Indeed, HSF2A is highly expressed in heat stressed plants. The thick line of the boxplots show the *median* HSF2A expression in both groups. The median is a measure of central tendency. You will also see that the individual datapoints are dispersed around the boxplot: quantifying this gives us a measure of the *spread* of the data. An example of such a measure of spread is the *standard deviation*. It looks like our heat stressed plants display a more narrow spread of the data, compared to the plants grown in the control condition.

::: {.callout-tip title="Question" icon="false"}
Can you name another measure of central tendency? And another one for the measure of spread?
:::

::: {.callout-caution title="Answer" collapse="true" icon="false"}
Another often used measure of central tendency would be the *mean*. As for the measure of spread, the variance (square of the standard deviation) or the standard error (standard deviation divided by the square root of the number of observations) are often used. 
:::

Let's test whether the *means* of the two groups are equal or not. We do this with a **two-sample *t*-test**.

```{R filename = "R"}
#| warning: false

t.test(x = control$expression,
       y = heat_stress$expression,
       alternative = "two.sided" ,      
       var.equal = FALSE,             # important as the variance do not seem equal
       conf.level = 0.95)             # this corresponds to alpha = 0.05 
```

Because the **p-value < 0.05**, we can safely reject our null hypothesis, and claim that we have discovered that *Arabidopsis* significantly upregulates the expression of HSF2A under heat stress. Sounds like it's time to write this down and submit a manuscript to *Nature*!

### Statistical power

Statistical power is the ability of your experimental sample population to detect differences (in this case, gene expression differences) that actually exist in the real population of your study species out there in the world. There are two main error one could make when testing hypotheses. Type I errors occur when the null hypothesis is rejected wrongly --- you detect a significant difference while in reality there is no difference in gene expression ("false positive"). Type II errors also are common in hypothesis testing --- they arise if you accept the null hypothesis when in fact a treatment effect exists ("false negative")

The power of an experiments is affected by several factors. We have seen so far:

- **The number of replicates**: more samples results in a higher power
- **Size of the effect**: bigger differences between groups are easier to detect
- **Variability of the data**: less noise is more power
- **Confidence level threshold**: usually, this is 0.05, but we can make our analysis more or less stringent by changing this to 0.01 or 0.1

## What determines the power of an RNA-seq experiment?

RNA-seq experiments measure the expression levels of thousands of genes simultaneously, meaning we perform thousands of statistical tests at once. To avoid making too many false discoveries (type I errors), we have to correct for multiple testing. However, it also reduces statistical power --- increasing the chance of missing real differences (type II errors). As a result, RNA-seq experiments can have lower power, especially for detecting small or subtle changes in gene expression. 

So, how do we deal with that? We have a few parameters to play with when designing an RNA-seq experiment, including deciding the number of reads per sample, and the number of samples per treatment. [Liu et al., 2014](https://academic.oup.com/bioinformatics/article/30/3/301/228651) investigated this systematically in an RNA-seq experiment with a human cell line. An exerpt from figure 1 is show below:

![](../images/02-de-replicates-2.png){width=5in}

*Image credits*: [Liu et al., 2014](https://academic.oup.com/bioinformatics/article/30/3/301/228651)

The figure shows the relationship between sequencing depth and number of replicates on the number of differentially expressed genes (DEGs) identified. It illustrates that **biological replicates are of greater importance than sequencing depth**. Above 10M-15M reads per sample, the number of DEGs increases marginally, while adding more biological replicates tends to return more DEGs. A similar conclusion was also reached by [Schurch et al., 2016](https://pmc.ncbi.nlm.nih.gov/articles/PMC4878611/). So, if your experiment is limited by a certain sequencing budget, it is almost always better to add more replicates than to sequence more reads of a limited number of samples. However, there are some caveats here. Say you are interested in the transcriptome of a fungal pathogen growing in plant roots. In such samples, most reads will probably come from the plant roots, so you will need to sequence deeply (that is, many reads from the same sample) to find DEGs from the fungal pathogen.

## Experimental design

In a typical biological experiment, you will encounter various sources of variation that are either:

- **Desirable** because they are part of your experimental factors. For example, the genotype, treatment, or timepoint.
- **Undesirable** because you are not interested in them. This could be: batch of RNA isolation, location of plants in the greenhouse, ...

Undesirable variation is unavaidable, but there are some practices to limit their impact. First of all, make sure that you don't confound your experimental factors with undesirable sources of variation by properly randomizing your treatments. For example, if you have too many samples to isolate RNA in one day of labwork, make sure you don't isolate RNA from samples with genotype A on day 1, and genotype B on day 2. In this case, you will never be able to know whether genes were differentially expressed because the genotypes of the samples differ (very interesting!) or because RNA was isolated on two different days (not very interesting). Instead, mix your samples of genotype A and B over your two RNA isolation days. Still, you should always record known sources of undesirable variation so you can correct for it later. There's much more to be said about this, for more details see [these materials from Harvard Chan Bioinformatics Core](https://hbctraining.github.io/Intro-to-rnaseq-fasrc-salmon-flipped/lessons/02_experimental_planning_considerations.html).


## Concluding remarks

In this section, we refreshed our statistics knowledge, and discussed how this applies to RNA-seq experiments. Hopefully, this enables you to craft a well-designed and controlled RNA-seq experiment. Now, head to the lab or greenhouse, perform your experiment, and extract high-quality RNA. How to do this will depend on your study system, and is beyond the scope of this workshop. We will see you again when you get the sequencing reads from the sequencing provider. We will then jump into the bioinformatics pipeline required to check the quality of the reads, and map the reads to the genome of your organisms of interest.
